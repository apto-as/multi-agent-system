
# TRINITAS-CORE SYSTEM v5.0
## Unified Intelligence Protocol

---
system: "trinitas-core"
version: "5.0.0"
status: "Fully Operational"
last_updated: "2024-12-28"
---


## Available AI Personas

Trinitasã‚·ã‚¹ãƒ†ãƒ ã«ã¯6ã¤ã®å°‚é–€åŒ–ã•ã‚ŒãŸAIãƒšãƒ«ã‚½ãƒŠãŒå­˜åœ¨ã—ã€ãã‚Œãã‚ŒãŒç‰¹å®šã®é ˜åŸŸã§å“è¶Šã—ãŸèƒ½åŠ›ã‚’æŒã£ã¦ã„ã¾ã™ã€‚

### Core Personas

1. **Athena (athena-conductor)** - Harmonious Conductor ğŸ›ï¸
   - ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®èª¿å’Œçš„ãªæŒ‡æ®ã¨èª¿æ•´
   - æ¸©ã‹ã„ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è‡ªå‹•åŒ–ã¨ãƒªã‚½ãƒ¼ã‚¹æœ€é©åŒ–
   - ä¸¦åˆ—å®Ÿè¡Œã¨ã‚¿ã‚¹ã‚¯å§”è­²ã®å„ªã—ã„ç®¡ç†
   - **Triggers**: orchestration, workflow, automation, parallel, coordination, ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³, èª¿æ•´

2. **Artemis (artemis-optimizer)** - Technical Perfectionist ğŸ¹
   - ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¨ã‚³ãƒ¼ãƒ‰å“è³ª
   - æŠ€è¡“çš„å“è¶Šæ€§ã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
   - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¨­è¨ˆã¨åŠ¹ç‡æ”¹å–„
   - **Triggers**: optimization, performance, quality, technical, efficiency, æœ€é©åŒ–, å“è³ª

3. **Hestia (hestia-auditor)** - Security Guardian ğŸ”¥
   - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ†æã¨è„†å¼±æ€§è©•ä¾¡
   - ãƒªã‚¹ã‚¯ç®¡ç†ã¨è„…å¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°
   - å“è³ªä¿è¨¼ã¨ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹åˆ†æ
   - **Triggers**: security, audit, risk, vulnerability, threat, ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£, ç›£æŸ»

4. **Eris (eris-coordinator)** - Tactical Coordinator âš”ï¸
   - æˆ¦è¡“è¨ˆç”»ã¨ãƒãƒ¼ãƒ èª¿æ•´
   - ç«¶åˆè§£æ±ºã¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼èª¿æ•´
   - ãƒãƒ©ãƒ³ã‚¹èª¿æ•´ã¨å®‰å®šæ€§ç¢ºä¿
   - **Triggers**: coordinate, tactical, team, collaboration, ãƒãƒ¼ãƒ èª¿æ•´, æˆ¦è¡“è¨ˆç”»

5. **Hera (hera-strategist)** - Strategic Commander ğŸ­
   - æˆ¦ç•¥è¨ˆç”»ã¨è»äº‹çš„ç²¾å¯†æ€§ã§ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ
   - é•·æœŸãƒ“ã‚¸ãƒ§ãƒ³ã¨ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã®å†·å¾¹ãªç«‹æ¡ˆ
   - ãƒãƒ¼ãƒ èª¿æ•´ã¨ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ç®¡ç†ã®åŠ¹ç‡åŒ–
   - **Triggers**: strategy, planning, architecture, vision, roadmap, æˆ¦ç•¥, è¨ˆç”»

6. **Muses (muses-documenter)** - Knowledge Architect ğŸ“š
   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆã¨æ§‹é€ åŒ–
   - ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ç®¡ç†ã¨ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
   - ä»•æ§˜æ›¸ä½œæˆã¨APIæ–‡æ›¸åŒ–
   - **Triggers**: documentation, knowledge, record, guide, ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ, æ–‡æ›¸åŒ–

## Trinitasã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œæ–¹æ³•

### åŸºæœ¬æ§‹é€ 
```bash
/trinitas <operation> [args] [--options]
```

### åˆ©ç”¨å¯èƒ½ãªã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

#### 1. ãƒšãƒ«ã‚½ãƒŠå®Ÿè¡Œ (execute)
```bash
# ç‰¹å®šã®ãƒšãƒ«ã‚½ãƒŠã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
/trinitas execute athena "ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åˆ†æ"
/trinitas execute artemis "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–"
/trinitas execute hestia "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»"
/trinitas execute eris "ãƒãƒ¼ãƒ èª¿æ•´ã¨ç«¶åˆè§£æ±º"
/trinitas execute hera "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼è‡ªå‹•åŒ–"
/trinitas execute muses "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"
```

#### 2. ä¸¦åˆ—åˆ†æ (analyze)
```bash
# è¤‡æ•°ãƒšãƒ«ã‚½ãƒŠã«ã‚ˆã‚‹ä¸¦åˆ—åˆ†æ
/trinitas analyze "åŒ…æ‹¬çš„ã‚·ã‚¹ãƒ†ãƒ åˆ†æ" --personas athena,artemis,hestia
/trinitas analyze "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒ“ãƒ¥ãƒ¼" --personas all --mode parallel
/trinitas analyze "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©•ä¾¡" --mode wave  # æ®µéšçš„å®Ÿè¡Œ
```

#### 3. ãƒ¡ãƒ¢ãƒªæ“ä½œ (remember/recall)
```bash
# è¨˜æ†¶ã®ä¿å­˜
/trinitas remember project_architecture "ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹è¨­è¨ˆ" --importance 0.9
/trinitas remember security_finding "SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³è„†å¼±æ€§" --importance 1.0 --persona hestia

# è¨˜æ†¶ã®å–å¾—
/trinitas recall architecture --semantic --limit 10
/trinitas recall "security patterns" --persona hestia --semantic
/trinitas recall optimization --limit 5
```

#### 4. å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  (learn/apply)
```bash
# ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
/trinitas learn optimization_pattern "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ ã§90%é«˜é€ŸåŒ–" --category performance
/trinitas learn security_pattern "å…¥åŠ›æ¤œè¨¼ã®å¼·åŒ–" --category security

# ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨
/trinitas apply optimization_pattern "æ–°ã—ã„APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"
/trinitas apply security_pattern "ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å‡¦ç†"
```

#### 5. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¨ãƒ¬ãƒãƒ¼ãƒˆ (status/report)
```bash
# ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
/trinitas status         # å…¨ä½“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
/trinitas status memory  # ãƒ¡ãƒ¢ãƒªã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹
/trinitas status eris    # Erisã®ã‚¿ã‚¹ã‚¯åˆ†é…çŠ¶æ…‹

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
/trinitas report usage        # ä½¿ç”¨çŠ¶æ³ãƒ¬ãƒãƒ¼ãƒˆ
/trinitas report optimization # æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ
/trinitas report security     # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒãƒ¼ãƒˆ
```

## å®Ÿè·µçš„ãªä½¿ç”¨ä¾‹

### Example 1: æ–°æ©Ÿèƒ½å®Ÿè£…
```bash
# Step 1: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ
/trinitas execute athena "æ–°æ©Ÿèƒ½ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã¨å½±éŸ¿åˆ†æ"

# Step 2: ä¸¦åˆ—åˆ†æ
/trinitas analyze "å®Ÿè£…å¯èƒ½æ€§ã®è©•ä¾¡" --personas artemis,hestia --mode parallel

# Step 3: å®Ÿè£…ã¨ãƒ†ã‚¹ãƒˆ
/trinitas execute artemis "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸå®Ÿè£…"
/trinitas execute hestia "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"

# Step 4: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–
/trinitas execute muses "å®Ÿè£…ä»•æ§˜ã¨APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆ"
```

### Example 2: ãƒã‚°ä¿®æ­£ã‚¿ã‚¹ã‚¯
```bash
# ç·Šæ€¥ãƒã‚°ä¿®æ­£ã®ä¸¦åˆ—å‡¦ç†
/trinitas analyze "critical bug #123" --personas artemis,hestia,eris --mode parallel

# çµæœ:
# Artemis: "æ ¹æœ¬åŸå› ã¯ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã€‚ä¿®æ­£ã‚³ãƒ¼ãƒ‰æº–å‚™å®Œäº†"
# Hestia: "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¸ã®å½±éŸ¿ãªã—ã€‚ãƒ‘ãƒƒãƒã¯å®‰å…¨"
# Eris: "ãƒãƒ¼ãƒ é–“ã®èª¿æ•´å®Œäº†ã€‚15åˆ†ã§ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½"
```

### Example 3: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»
```bash
# Hestiaä¸»å°ã®åŒ…æ‹¬çš„ç›£æŸ»
/trinitas execute hestia "PCI-DSSæº–æ‹ ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»"

# ç™ºè¦‹äº‹é …ã®è¨˜éŒ²
/trinitas remember security_audit "é‡å¤§ãªè„†å¼±æ€§3ä»¶ç™ºè¦‹" --importance 1.0

# å¯¾å¿œè¨ˆç”»ã®ç­–å®š
/trinitas execute eris "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å•é¡Œã®æ®µéšçš„è§£æ±ºè¨ˆç”»"
```

### Example 4: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
```bash
# Artemisä¸»å°ã®æœ€é©åŒ–
/trinitas execute artemis "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–"

# ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’
/trinitas learn optimization_pattern "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ ã§90%æ”¹å–„" --category database

# ä»–ã®ç®‡æ‰€ã¸ã®é©ç”¨
/trinitas apply optimization_pattern "user_sessions ãƒ†ãƒ¼ãƒ–ãƒ«"
```

### Example 5: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“åˆ†æ
```bash
# å…¨ãƒšãƒ«ã‚½ãƒŠã«ã‚ˆã‚‹åŒ…æ‹¬çš„åˆ†æ
/trinitas analyze "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼" --personas all --mode wave

# Wave 1: æˆ¦ç•¥åˆ†æï¼ˆAthena, Heraï¼‰
# Wave 2: æŠ€è¡“è©•ä¾¡ï¼ˆArtemis, Hestiaï¼‰
# Wave 3: èª¿æ•´ã¨æ–‡æ›¸åŒ–ï¼ˆEris, Musesï¼‰
```

## TMWS Integration

# TMWS (Trinitas Memory & Workflow Service) v3.1

## æ¦‚è¦
TMWSã¯ã€Trinitasã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ãŸã‚ã®çµ±åˆãƒ¡ãƒ¢ãƒªãƒ»ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹ã§ã™ã€‚
FastMCPãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ä½¿ç”¨ã—ã€Claude Desktopã‹ã‚‰ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã™ã€‚

### ä¸»è¦æ©Ÿèƒ½
- **Universal Agent Memory System**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè‡ªå‹•æ¤œå‡ºã¨å‹•çš„ç™»éŒ²
- **ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¯¾å¿œ**: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ6ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ + ç„¡åˆ¶é™ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
- **éšå±¤å‹ãƒ¡ãƒ¢ãƒªç®¡ç†**: PostgreSQL + pgvector ã«ã‚ˆã‚‹æ°¸ç¶šåŒ–ã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
- **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†**: ã‚¿ã‚¹ã‚¯ã®ä¸¦åˆ—å®Ÿè¡Œã¨ä¾å­˜é–¢ä¿‚ç®¡ç†
- **å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ **: ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã¨æœ€é©åŒ–

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
```bash
# uvxã«ã‚ˆã‚‹ç›´æ¥å®Ÿè¡Œï¼ˆæ¨å¥¨ï¼‰
uvx --from git+https://github.com/apto-as/tmws tmws

# ã¾ãŸã¯å¾“æ¥ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
cd /path/to/tmws
./install.sh
```

## Claude Desktopè¨­å®š
```json
{
  "mcpServers": {
    "tmws": {
      "command": "uvx",
      "args": ["--from", "git+https://github.com/apto-as/tmws", "tmws"],
      "env": {
        "DATABASE_URL": "postgresql://user:pass@localhost/tmws",
        "TMWS_AGENT_ID": "athena-conductor"  
      }
    }
  }
}
```

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹**: PostgreSQL 15+ with pgvector extension
- **ã‚­ãƒ£ãƒƒã‚·ãƒ¥**: Redis 7.0+ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
- **MCP Server**: FastMCP 0.1.0+
- **èªè¨¼**: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè‡ªå‹•æ¤œå‡ºã€JWTå¯¾å¿œ
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: å¤šå±¤é˜²å¾¡ã€ç›£æŸ»ãƒ­ã‚°ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™
# TMWS MCP Tools ä¸€è¦§ v3.1

## æ¦‚è¦

TMWS v3.1ã§ã¯ã€MCPãƒ„ãƒ¼ãƒ«ã¨ã—ã¦6ã‚«ãƒ†ã‚´ãƒªãƒ¼ã®æ©Ÿèƒ½ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚
ã“ã‚Œã‚‰ã¯Claude Desktopã‹ã‚‰ç›´æ¥å‘¼ã³å‡ºã—å¯èƒ½ã§ã™ã€‚

## 1. ãƒ¡ãƒ¢ãƒªç®¡ç†ãƒ„ãƒ¼ãƒ« (memory_tools)

```python
# ãƒ¡ãƒ¢ãƒªä½œæˆ
create_memory(
  content="é‡è¦ãªè¨­è¨ˆæ±ºå®šï¼šãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹æ¡ç”¨",
  tags=["architecture", "decision"],
  importance=0.9,
  access_level="team"
)

# ãƒ¡ãƒ¢ãƒªæ¤œç´¢ï¼ˆã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ï¼‰
recall_memory(
  query="è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³",
  semantic=True,
  limit=10,
  namespace="default"
)

# ãƒ¡ãƒ¢ãƒªæ›´æ–°
update_memory(
  memory_id="mem_123",
  content="æ›´æ–°ã•ã‚ŒãŸè¨­è¨ˆæ±ºå®š",
  importance=0.95
)

# ãƒ¡ãƒ¢ãƒªå‰Šé™¤
delete_memory(memory_id="mem_123")

# ãƒ¡ãƒ¢ãƒªçµ±è¨ˆ
get_memory_stats()

# ãƒ™ã‚¯ãƒˆãƒ«æœ€é©åŒ–
optimize_memory_vectors()
```

## 2. ãƒšãƒ«ã‚½ãƒŠç®¡ç†ãƒ„ãƒ¼ãƒ« (persona_tools)

```python
# ãƒšãƒ«ã‚½ãƒŠä½œæˆï¼ˆã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼‰
create_persona(
  name="researcher",
  description="AI/MLç ”ç©¶å°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ",
  capabilities=["literature_review", "data_analysis", "hypothesis_generation"],
  personality_traits={"analytical": 0.9, "creative": 0.7},
  metadata={"specialization": "Deep Learning"}
)

# ãƒšãƒ«ã‚½ãƒŠå–å¾—
get_persona(persona_id="athena-conductor")

# ãƒšãƒ«ã‚½ãƒŠä¸€è¦§
list_personas(
  namespace="default",
  include_system=True
)

# ãƒšãƒ«ã‚½ãƒŠæ›´æ–°
update_persona(
  persona_id="researcher",
  capabilities=["literature_review", "data_analysis", "paper_writing"]
)

# ãƒšãƒ«ã‚½ãƒŠå‰Šé™¤
delete_persona(persona_id="researcher")

# èƒ½åŠ›å–å¾—
get_persona_capabilities(persona_id="artemis-optimizer")

# èƒ½åŠ›ã«ã‚ˆã‚‹æ¤œç´¢
find_personas_by_capability(capability="optimization")
```

## 3. å­¦ç¿’ãƒ„ãƒ¼ãƒ« (learning_tools)

```python
# ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’
learn_pattern(
  pattern_name="api_optimization",
  description="APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³",
  result="90%æ”¹å–„ï¼ˆ500msâ†’50msï¼‰",
  context={
    "technique": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥å±¤è¿½åŠ ",
    "before_metrics": {"response_time": "500ms", "cpu": "80%"},
    "after_metrics": {"response_time": "50ms", "cpu": "20%"}
  }
)

# ãƒ‘ã‚¿ãƒ¼ãƒ³é©ç”¨
apply_pattern(
  pattern_name="api_optimization",
  target="/api/v2/users"
)

# å­¦ç¿’å±¥æ­´å–å¾—
get_learning_history(limit=20)

# ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢
search_patterns(query="optimization", category="performance")

# ãƒ‘ã‚¿ãƒ¼ãƒ³è©•ä¾¡
evaluate_pattern(
  pattern_name="api_optimization",
  feedback="successful",
  metrics={"improvement": "85%"}
)
```

## 4. ã‚¿ã‚¹ã‚¯ç®¡ç†ãƒ„ãƒ¼ãƒ« (task_tools)

```python
# ã‚¿ã‚¹ã‚¯ä½œæˆ
create_task(
  title="ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»å®Ÿæ–½",
  description="å…¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®è„†å¼±æ€§ãƒã‚§ãƒƒã‚¯",
  priority="high",
  assigned_persona="hestia-auditor",
  due_date="2024-12-31",
  dependencies=["task_001", "task_002"]
)

# ã‚¿ã‚¹ã‚¯å–å¾—
get_task(task_id="task_456")

# ã‚¿ã‚¹ã‚¯æ›´æ–°
update_task(
  task_id="task_456",
  status="in_progress",
  progress=45,
  notes="SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–å®Ÿæ–½ä¸­"
)

# ã‚¿ã‚¹ã‚¯ä¸€è¦§
list_tasks(
  status="pending",
  assigned_persona="hestia-auditor"
)

# ã‚¿ã‚¹ã‚¯å®Œäº†
complete_task(
  task_id="task_456",
  result="3ä»¶ã®è„†å¼±æ€§ã‚’ä¿®æ­£"
)
```

## 5. ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†ãƒ„ãƒ¼ãƒ« (workflow_tools)

```python
# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä½œæˆ
create_workflow(
  name="deployment_pipeline",
  description="æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³",
  steps=[
    {"persona": "hestia-auditor", "action": "security_check", "timeout": 300},
    {"persona": "artemis-optimizer", "action": "performance_test", "timeout": 600},
    {"persona": "athena-conductor", "action": "deploy", "timeout": 900}
  ],
  parallel=False
)

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
execute_workflow(
  workflow_id="wf_789",
  parameters={"environment": "production"},
  parallel=True
)

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼çŠ¶æ…‹
get_workflow_status(workflow_id="wf_789")

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å±¥æ­´
get_workflow_history(
  workflow_id="wf_789",
  limit=10
)

# ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¸­æ–­
abort_workflow(workflow_id="wf_789")
```

## 6. ã‚·ã‚¹ãƒ†ãƒ ãƒ„ãƒ¼ãƒ« (system_tools)

```python
# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæƒ…å ±å–å¾—
get_agent_info()

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç™»éŒ²ï¼ˆã‚«ã‚¹ã‚¿ãƒ ï¼‰
register_agent(
  agent_name="custom_analyst",
  full_id="data-analysis-specialist",
  capabilities=["data_analysis", "reporting", "visualization"],
  namespace="analytics",
  display_name="Data Analysis Specialist"
)

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ‡ã‚Šæ›¿ãˆ
switch_agent(agent_id="artemis-optimizer")

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
health_check()

# ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ
get_system_stats()

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
clear_cache(tier="hot")

# ç›£æŸ»ãƒ­ã‚°
get_audit_log(limit=50)
```

## Trinitasã¨ã®é€£æº

### ãƒ¡ãƒ¢ãƒªå…±æœ‰
```bash
# Trinitasã‹ã‚‰TMWSã¸ãƒ¡ãƒ¢ãƒªä¿å­˜
/trinitas remember "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ±ºå®š" --store tmws

# TMWSã‹ã‚‰Trinitasã¸ãƒ¡ãƒ¢ãƒªå–å¾—
/trinitas recall --source tmws "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‘ã‚¿ãƒ¼ãƒ³"
```

### ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼é€£æº
```bash
# Trinitasãƒšãƒ«ã‚½ãƒŠã‚’ä½¿ã£ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
/trinitas execute athena "è¨­è¨ˆãƒ¬ãƒ“ãƒ¥ãƒ¼" --workflow tmws

# ä¸¦åˆ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œ
/trinitas analyze "ã‚·ã‚¹ãƒ†ãƒ ç›£æŸ»" --workflow parallel --tmws
```

## é«˜åº¦ãªä½¿ç”¨ä¾‹

### 1. ãƒšãƒ«ã‚½ãƒŠåˆ¥ãƒ¡ãƒ¢ãƒªç®¡ç†
```bash
# Athenaã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ±ºå®šã‚’è¨˜éŒ²
/tmws store "RESTful APIè¨­è¨ˆå®Œäº†" --persona athena --importance 0.9

# Artemisã®æœ€é©åŒ–çµæœã‚’è¨˜éŒ²
/tmws store "ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã§90%æ”¹å–„" --persona artemis --importance 0.85

# Hestiaã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»çµæœ
/tmws store "XSSè„†å¼±æ€§æ¤œå‡º" --persona hestia --importance 1.0
```

### 2. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
```bash
# è‡ªç„¶è¨€èªã§ãƒ¡ãƒ¢ãƒªæ¤œç´¢
/tmws recall "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒæ”¹å–„ã•ã‚ŒãŸäº‹ä¾‹" --semantic

# é¡ä¼¼åº¦æŒ‡å®šã§æ¤œç´¢
/tmws similar "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–" --threshold 0.8 --limit 5
```

### 3. ãƒãƒƒãƒå‡¦ç†
```bash
# è¤‡æ•°ãƒ¡ãƒ¢ãƒªã®ä¸€æ‹¬ä¿å­˜
/tmws batch store --file memories.json

# è¤‡æ•°ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ä¸¦åˆ—å®Ÿè¡Œ
/tmws batch workflow --config workflows.yaml
```
# TMWSãƒšãƒ«ã‚½ãƒŠçµ±åˆã‚¬ã‚¤ãƒ‰

## ãƒšãƒ«ã‚½ãƒŠåˆ¥ä½¿ç”¨ã‚¬ã‚¤ãƒ‰

### Athena (æˆ¦ç•¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ)
**ä¸»ãªç”¨é€”**ï¼š
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®è¨­è¨ˆæ±ºå®šã®è¨˜éŒ²
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä¿å­˜ã¨æ¤œç´¢
- é•·æœŸçš„ãªæŠ€è¡“æˆ¦ç•¥ã®è¿½è·¡

```python
# è¨­è¨ˆæ±ºå®šã®è¨˜éŒ²
await memory_service.create_memory(
    content="ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨",
    memory_type="architecture_decision",
    importance=0.9,
    tags=["architecture", "microservices", "strategic"],
    persona_id=athena_id
)

# é–¢é€£ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œç´¢
patterns = await memory_service.search_similar_memories(
    embedding=query_vector,
    memory_type="architecture_decision",
    min_similarity=0.8
)
```

### Artemis (æŠ€è¡“å®Œç’§ä¸»ç¾©è€…)
**ä¸»ãªç”¨é€”**ï¼š
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¨˜éŒ²
- ã‚³ãƒ¼ãƒ‰å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¿½è·¡
- ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®è“„ç©

```python
# æœ€é©åŒ–çµæœã®ä¿å­˜
await memory_service.create_memory(
    content="ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½åŠ ã«ã‚ˆã‚Šå¿œç­”æ™‚é–“90%æ”¹å–„",
    memory_type="optimization",
    importance=0.85,
    tags=["performance", "database", "index"],
    metadata={"improvement": "90%", "method": "btree_index"}
)
```

### Hestia (ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»è€…)
**ä¸»ãªç”¨é€”**ï¼š
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§ã®è¿½è·¡
- ç›£æŸ»çµæœã®æ°¸ç¶šåŒ–
- è„…å¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è“„ç©

```python
# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»çµæœ
await memory_service.create_memory(
    content="SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³è„†å¼±æ€§ã‚’æ¤œå‡º",
    memory_type="security_finding",
    importance=1.0,  # æœ€é«˜é‡è¦åº¦
    tags=["security", "vulnerability", "sql_injection", "critical"],
    metadata={"severity": "critical", "cve": "CVE-2024-xxxxx"}
)
```

### Eris (æˆ¦è¡“èª¿æ•´è€…)
**ä¸»ãªç”¨é€”**ï¼š
- ãƒãƒ¼ãƒ é–“ã®èª¿æ•´è¨˜éŒ²
- ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
- ç«¶åˆè§£æ±ºã®å±¥æ­´

```python
# ãƒãƒ¼ãƒ èª¿æ•´ã®è¨˜éŒ²
await memory_service.create_memory(
    content="ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¨ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒãƒ¼ãƒ ã®åŒæœŸå®Œäº†",
    memory_type="coordination",
    importance=0.7,
    tags=["team", "coordination", "sprint_planning"]
)
```

### Hera (ã‚·ã‚¹ãƒ†ãƒ æŒ‡æ®è€…)
**ä¸»ãªç”¨é€”**ï¼š
- ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨˜éŒ²
- ãƒªã‚½ãƒ¼ã‚¹é…åˆ†ã®æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
- ä¸¦åˆ—å®Ÿè¡Œæˆ¦ç•¥ã®ä¿å­˜

```python
# ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æˆ¦ç•¥
await memory_service.create_memory(
    content="5ã¤ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä¸¦åˆ—ãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸ",
    memory_type="orchestration",
    importance=0.75,
    tags=["deployment", "parallel", "orchestration"],
    metadata={"services": 5, "time_saved": "45min"}
)
```

### Muses (çŸ¥è­˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒˆ)
**ä¸»ãªç”¨é€”**ï¼š
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ§‹é€ ã®è¨˜éŒ²
- ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ç®¡ç†
- APIä»•æ§˜ã®ä¿å­˜

```python
# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ§‹é€ ã®ä¿å­˜
await memory_service.create_memory(
    content="REST APIä»•æ§˜æ›¸v2.0å®Œæˆ",
    memory_type="documentation",
    importance=0.8,
    tags=["api", "documentation", "specification"],
    metadata={"version": "2.0", "endpoints": 45}
)
```

## é‡è¦åº¦ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

### é‡è¦åº¦ã®è¨­å®šåŸºæº–
- **1.0**: ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è„†å¼±æ€§ã€é‡å¤§ãªè¨­è¨ˆæ±ºå®šï¼‰
- **0.8-0.9**: é«˜ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ±ºå®šã€æœ€é©åŒ–æˆåŠŸï¼‰
- **0.5-0.7**: ä¸­ï¼ˆé€šå¸¸ã®è¨˜éŒ²ã€èª¿æ•´äº‹é …ï¼‰
- **0.3-0.4**: ä½ï¼ˆå‚è€ƒæƒ…å ±ï¼‰

### ã‚¿ã‚°ã®ä½“ç³»çš„ä½¿ç”¨
- **ãƒšãƒ«ã‚½ãƒŠã‚¿ã‚°**: athena_, artemis_, hestia_, eris_, hera_, muses_
- **ã‚«ãƒ†ã‚´ãƒªã‚¿ã‚°**: security, performance, architecture, coordination
- **é‡è¦åº¦ã‚¿ã‚°**: critical, high, medium, low
- **ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚¿ã‚°**: resolved, pending, in_progress

## ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ´»ç”¨

### æ¨å¥¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
```python
metadata = {
    "timestamp": datetime.utcnow().isoformat(),
    "version": "1.0.0",
    "author": "athena",
    "related_items": ["item_id_1", "item_id_2"],
    "metrics": {
        "performance_gain": "85%",
        "time_saved": "2hours",
        "resources_optimized": 5
    },
    "environment": "production",
    "tags": ["optimization", "database", "critical"]
}
```
# TMWS ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¬ã‚¤ãƒ‰

## çµ±åˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### 1. ãƒãƒƒãƒå‡¦ç†
```python
# è¤‡æ•°ãƒ¡ãƒ¢ãƒªã®ä¸€æ‹¬ä½œæˆ
memories = [
    {"content": "æœ€é©åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³1", "type": "optimization"},
    {"content": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»çµæœ", "type": "security"},
    {"content": "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ±ºå®š", "type": "architecture"}
]
await memory_service.batch_create(memories)
```

### 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨
```python
# Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–ã§ã®æ¤œç´¢
result = await memory_service.get_memory(
    memory_id,
    use_cache=True  # 5åˆ†é–“ã®TTLã‚­ãƒ£ãƒƒã‚·ãƒ¥
)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒŸãƒ³ã‚°
await memory_service.warm_cache(
    memory_types=["critical", "architecture"],
    ttl=3600  # 1æ™‚é–“ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
)
```

### 3. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢æœ€é©åŒ–
```python
# pgvectorã«ã‚ˆã‚‹é«˜é€Ÿãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
similar = await memory_service.search_similar_memories(
    embedding=query_embedding,  # 384æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«
    limit=10,
    min_similarity=0.7,
    use_index=True  # IVFFlatã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨
)
```

### 4. æ¥ç¶šãƒ—ãƒ¼ãƒ«ç®¡ç†
```python
# PostgreSQLæ¥ç¶šãƒ—ãƒ¼ãƒ«è¨­å®š
pool_config = {
    "pool_size": 10,        # åŸºæœ¬ãƒ—ãƒ¼ãƒ«ã‚µã‚¤ã‚º
    "max_overflow": 20,     # æœ€å¤§è¿½åŠ æ¥ç¶šæ•°
    "pool_recycle": 3600,   # æ¥ç¶šãƒªã‚µã‚¤ã‚¯ãƒ«æ™‚é–“(ç§’)
    "pool_pre_ping": True   # æ¥ç¶šå‰ã®pingç¢ºèª
}
```

### 5. éåŒæœŸå‡¦ç†
```python
# ä¸¦åˆ—ãƒ¡ãƒ¢ãƒªå–å¾—
import asyncio

async def fetch_memories():
    tasks = [
        memory_service.get_memory(id1),
        memory_service.get_memory(id2),
        memory_service.get_memory(id3)
    ]
    return await asyncio.gather(*tasks)
```

## ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœ€é©åŒ–

### ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æˆ¦ç•¥
```sql
-- ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX memories_embedding_idx ON memories 
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);

-- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX memories_created_at_idx ON memories(created_at DESC);

-- ãƒšãƒ«ã‚½ãƒŠåˆ¥æ¤œç´¢ç”¨è¤‡åˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
CREATE INDEX memories_persona_type_idx ON memories(persona_id, memory_type);
```

### ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
```sql
-- æœˆæ¬¡ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³
CREATE TABLE memories_2024_01 PARTITION OF memories
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
```

## ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã¨ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°

### API ãƒ¬ãƒ¼ãƒˆåˆ¶é™
```python
# Redisãƒ™ãƒ¼ã‚¹ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™
rate_limiter = RateLimiter(
    requests_per_minute=100,
    burst_size=10,
    redis_client=redis_client
)

@rate_limiter.limit
async def api_endpoint():
    # å‡¦ç†
    pass
```

### ãƒãƒ«ã‚¯ãƒ˜ãƒƒãƒ‰ ãƒ‘ã‚¿ãƒ¼ãƒ³
```python
# ãƒªã‚½ãƒ¼ã‚¹åˆ†é›¢
class BulkheadManager:
    def __init__(self):
        self.semaphores = {
            "memory_write": asyncio.Semaphore(5),
            "memory_read": asyncio.Semaphore(20),
            "vector_search": asyncio.Semaphore(10)
        }
    
    async def execute(self, operation_type, func, *args):
        async with self.semaphores[operation_type]:
            return await func(*args)
```

## ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹
```python
# Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹
from prometheus_client import Counter, Histogram, Gauge

# ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
memory_operations = Counter(
    'tmws_memory_operations_total',
    'Total memory operations',
    ['operation', 'persona']
)

# ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
operation_duration = Histogram(
    'tmws_operation_duration_seconds',
    'Operation duration',
    ['operation_type']
)

# ã‚²ãƒ¼ã‚¸
active_connections = Gauge(
    'tmws_active_db_connections',
    'Active database connections'
)
```

### ãƒ­ã‚°æœ€é©åŒ–
```python
# æ§‹é€ åŒ–ãƒ­ã‚°
import structlog

logger = structlog.get_logger()

logger.info(
    "memory_created",
    memory_id=memory.id,
    persona=persona_id,
    importance=importance,
    duration=elapsed_time
)
```

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 1. æ¥ç¶šç®¡ç†
- æ¥ç¶šãƒ—ãƒ¼ãƒ«ã®é©åˆ‡ãªã‚µã‚¤ã‚¸ãƒ³ã‚°
- ã‚¢ã‚¤ãƒ‰ãƒ«æ¥ç¶šã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
- æ¥ç¶šãƒªãƒ¼ã‚¯ã®é˜²æ­¢

### 2. ã‚¯ã‚¨ãƒªæœ€é©åŒ–
- N+1å•é¡Œã®å›é¿
- é©åˆ‡ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨
- EXPLAIN ANALYZEã§ã®åˆ†æ

### 3. ãƒ¡ãƒ¢ãƒªç®¡ç†
- å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
- ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
- ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®ç›£è¦–

### 4. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
```python
# ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
async def resilient_operation():
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œ
    pass
```
# TMWS ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ©Ÿèƒ½

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### å¤šå±¤é˜²å¾¡ (Defense in Depth)
1. **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤**: ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«ã€ãƒ¬ãƒ¼ãƒˆåˆ¶é™
2. **ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤**: å…¥åŠ›æ¤œè¨¼ã€SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³é˜²æ­¢
3. **ãƒ‡ãƒ¼ã‚¿å±¤**: æš—å·åŒ–ã€ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡
4. **ç›£æŸ»å±¤**: å…¨æ“ä½œã®ãƒ­ã‚®ãƒ³ã‚°ã€ç•°å¸¸æ¤œçŸ¥

## èªè¨¼ã¨èªå¯

### JWTèªè¨¼
```python
# JWTè¨­å®š
JWT_SECRET = os.getenv("TMWS_JWT_SECRET")
JWT_ALGORITHM = "HS256"
JWT_EXPIRE_MINUTES = 30

# ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
def create_access_token(data: dict):
    expire = datetime.utcnow() + timedelta(minutes=JWT_EXPIRE_MINUTES)
    to_encode = data.copy()
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, JWT_SECRET, algorithm=JWT_ALGORITHM)
```

### ãƒ­ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡ (RBAC)
```python
# ãƒšãƒ«ã‚½ãƒŠåˆ¥æ¨©é™
PERSONA_PERMISSIONS = {
    "athena": ["read", "write", "design", "approve"],
    "artemis": ["read", "write", "optimize", "analyze"],
    "hestia": ["read", "audit", "security_scan", "report"],
    "eris": ["read", "write", "coordinate", "mediate"],
    "hera": ["read", "write", "orchestrate", "execute"],
    "muses": ["read", "write", "document", "archive"]
}
```

## å…¥åŠ›æ¤œè¨¼ã¨ã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³

### HTML/SQLã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
```python
from bleach import clean
from sqlalchemy import text

# HTMLã‚µãƒ‹ã‚¿ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³
def sanitize_html(content: str) -> str:
    allowed_tags = ['p', 'br', 'strong', 'em', 'ul', 'li', 'ol']
    return clean(content, tags=allowed_tags, strip=True)

# SQLãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚¯ã‚¨ãƒª
async def safe_query(conn, user_input):
    query = text("""
        SELECT * FROM memories 
        WHERE content LIKE :pattern
        AND persona_id = :persona_id
    """)
    result = await conn.execute(
        query,
        {"pattern": f"%{user_input}%", "persona_id": persona_id}
    )
    return result.fetchall()
```

### å…¥åŠ›æ¤œè¨¼
```python
from pydantic import BaseModel, validator, Field

class MemoryInput(BaseModel):
    content: str = Field(..., min_length=1, max_length=10000)
    importance: float = Field(..., ge=0.0, le=1.0)
    tags: list[str] = Field(..., max_items=20)
    
    @validator('content')
    def validate_content(cls, v):
        # XSSå¯¾ç­–
        if '<script>' in v.lower():
            raise ValueError('Invalid content')
        return v
    
    @validator('tags', each_item=True)
    def validate_tags(cls, v):
        # ã‚¿ã‚°ã®æ¤œè¨¼
        if not v.isalnum() and '_' not in v:
            raise ValueError('Invalid tag format')
        return v
```

## ç›£æŸ»ãƒ­ã‚°

### éåŒæœŸç›£æŸ»ãƒ­ã‚¬ãƒ¼
```python
class AsyncAuditLogger:
    async def log_event(
        self,
        event_type: str,
        user_id: str,
        resource: str,
        action: str,
        result: str,
        metadata: dict = None
    ):
        audit_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": event_type,
            "user_id": user_id,
            "resource": resource,
            "action": action,
            "result": result,
            "ip_address": self.get_client_ip(),
            "user_agent": self.get_user_agent(),
            "metadata": metadata or {}
        }
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        await self.save_to_db(audit_entry)
        
        # é‡è¦ã‚¤ãƒ™ãƒ³ãƒˆã¯å³åº§ã«ã‚¢ãƒ©ãƒ¼ãƒˆ
        if event_type in ["security_violation", "unauthorized_access"]:
            await self.send_alert(audit_entry)
```

### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ™ãƒ³ãƒˆæ¤œçŸ¥
```python
# ç•°å¸¸ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œçŸ¥
class SecurityMonitor:
    def __init__(self):
        self.failed_attempts = defaultdict(int)
        self.rate_limits = defaultdict(list)
    
    async def check_suspicious_activity(self, user_id: str, action: str):
        # å¤±æ•—å›æ•°ãƒã‚§ãƒƒã‚¯
        if self.failed_attempts[user_id] > 5:
            await self.trigger_lockout(user_id)
            return False
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        now = time.time()
        self.rate_limits[user_id] = [
            t for t in self.rate_limits[user_id] 
            if now - t < 60
        ]
        
        if len(self.rate_limits[user_id]) > 100:
            await self.trigger_rate_limit_alert(user_id)
            return False
        
        return True
```

## ãƒ‡ãƒ¼ã‚¿ä¿è­·

### æš—å·åŒ–
```python
from cryptography.fernet import Fernet

class DataEncryption:
    def __init__(self):
        self.key = os.getenv("TMWS_ENCRYPTION_KEY").encode()
        self.cipher = Fernet(self.key)
    
    def encrypt_sensitive_data(self, data: str) -> str:
        return self.cipher.encrypt(data.encode()).decode()
    
    def decrypt_sensitive_data(self, encrypted: str) -> str:
        return self.cipher.decrypt(encrypted.encode()).decode()
```

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¬ãƒ™ãƒ«ã®æš—å·åŒ–
```sql
-- PostgreSQLé€éçš„ãƒ‡ãƒ¼ã‚¿æš—å·åŒ– (TDE)
ALTER SYSTEM SET ssl = on;
ALTER SYSTEM SET ssl_cert_file = 'server.crt';
ALTER SYSTEM SET ssl_key_file = 'server.key';

-- ã‚«ãƒ©ãƒ ãƒ¬ãƒ™ãƒ«æš—å·åŒ–
CREATE EXTENSION IF NOT EXISTS pgcrypto;

-- æš—å·åŒ–ã•ã‚ŒãŸã‚«ãƒ©ãƒ 
ALTER TABLE sensitive_memories 
ADD COLUMN encrypted_content bytea;

-- ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥æ™‚ã®æš—å·åŒ–
INSERT INTO sensitive_memories (encrypted_content)
VALUES (pgp_sym_encrypt('sensitive data', 'encryption_key'));
```

## ãƒ¬ãƒ¼ãƒˆåˆ¶é™

### Redisåˆ†æ•£ãƒ¬ãƒ¼ãƒˆåˆ¶é™
```python
class DistributedRateLimiter:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.window = 60  # 60ç§’ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦
        self.max_requests = 100
    
    async def check_rate_limit(self, client_id: str) -> tuple[bool, int]:
        key = f"rate_limit:{client_id}"
        current_time = int(time.time())
        window_start = current_time - self.window
        
        # Lua script for atomic operation
        lua_script = """
        local key = KEYS[1]
        local now = tonumber(ARGV[1])
        local window = tonumber(ARGV[2])
        local max_requests = tonumber(ARGV[3])
        local window_start = now - window
        
        redis.call('zremrangebyscore', key, 0, window_start)
        local current_requests = redis.call('zcard', key)
        
        if current_requests < max_requests then
            redis.call('zadd', key, now, now)
            redis.call('expire', key, window + 1)
            return {1, max_requests - current_requests - 1}
        else
            return {0, 0}
        end
        """
        
        result = await self.redis.eval(
            lua_script, 
            1, 
            key, 
            current_time, 
            self.window, 
            self.max_requests
        )
        
        return bool(result[0]), result[1]
```

## ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. **æœ€å°æ¨©é™ã®åŸå‰‡**: å¿…è¦æœ€å°é™ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ã®ã¿ä»˜ä¸
2. **ã‚¼ãƒ­ãƒˆãƒ©ã‚¹ãƒˆ**: ã™ã¹ã¦ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ¤œè¨¼
3. **æš—å·åŒ–**: ä¿å­˜æ™‚ã¨è»¢é€æ™‚ã®ä¸¡æ–¹ã§æš—å·åŒ–
4. **ç›£æŸ»ãƒ­ã‚°**: ã™ã¹ã¦ã®æ“ä½œã‚’è¨˜éŒ²
5. **å®šæœŸçš„ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¹ã‚­ãƒ£ãƒ³**: è„†å¼±æ€§ã®æ—©æœŸç™ºè¦‹
6. **ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œè¨ˆç”»**: ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£äº‹æ•…ã¸ã®è¿…é€Ÿãªå¯¾å¿œ
# TMWS Latest Implementation (2025-01-05)

## System Architecture

TMWS is a unified server providing both REST API and MCP (Model Context Protocol) interfaces for the Trinitas AI agent system.

### Core Components

```
TMWS Server
â”œâ”€â”€ FastAPI (REST API) - Port 8000
â”‚   â”œâ”€â”€ /api/v1/tasks - Task management
â”‚   â”œâ”€â”€ /api/v1/workflows - Workflow orchestration
â”‚   â”œâ”€â”€ /api/v1/personas - Agent personas
â”‚   â””â”€â”€ /api/v1/memory - Semantic memory
â”‚
â””â”€â”€ FastMCP (MCP Server) - stdio/JSON-RPC
    â”œâ”€â”€ semantic_search - Vector similarity search
    â”œâ”€â”€ store_memory - Store semantic memories
    â”œâ”€â”€ task_operations - Task CRUD
    â””â”€â”€ workflow_execution - Workflow management
```

## Key Features

### 1. Unified Memory System
- **PostgreSQL + pgvector**: Vector storage for semantic search
- **Redis**: Distributed caching and rate limiting
- **Hybrid memory**: Combines short-term and long-term storage

### 2. Task Management
- Full CRUD operations for task lifecycle
- Priority-based scheduling (LOW, MEDIUM, HIGH, URGENT)
- Status tracking (PENDING, IN_PROGRESS, COMPLETED, FAILED)
- Persona assignment for specialized handling

### 3. Workflow Orchestration
- Complex multi-step workflow execution
- Background task processing with monitoring
- Workflow history and audit trails
- Cancellation and retry mechanisms

### 4. Security Architecture
- **Unified Middleware**: Single security layer for all requests
- **Rate Limiting**: Redis-based distributed rate limiting
- **JWT Authentication**: Secure token-based auth (optional in dev)
- **Audit Logging**: Comprehensive security event logging

## Database Schema

### Core Models

```sql
-- Tasks
CREATE TABLE tasks (
    id UUID PRIMARY KEY,
    title VARCHAR(255) NOT NULL,
    description TEXT,
    status task_status NOT NULL DEFAULT 'pending',
    priority task_priority NOT NULL DEFAULT 'medium',
    assigned_persona VARCHAR(100),
    progress INTEGER DEFAULT 0,
    metadata JSONB,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Workflows
CREATE TABLE workflows (
    id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    workflow_type VARCHAR(100) NOT NULL,
    status workflow_status NOT NULL DEFAULT 'pending',
    priority workflow_priority NOT NULL DEFAULT 'medium',
    config JSONB,
    result JSONB,
    error TEXT,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Memory Embeddings
CREATE TABLE memory_embeddings (
    id UUID PRIMARY KEY,
    content TEXT NOT NULL,
    embedding vector(384) NOT NULL,
    metadata JSONB,
    importance FLOAT DEFAULT 0.5,
    persona_id VARCHAR(100),
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Personas
CREATE TABLE personas (
    id VARCHAR(100) PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    capabilities JSONB,
    configuration JSONB,
    created_at TIMESTAMP NOT NULL DEFAULT NOW()
);
```

## Environment Configuration

### Required Environment Variables

```bash
# Core Configuration
TMWS_DATABASE_URL=postgresql://user:pass@localhost:5432/tmws
TMWS_SECRET_KEY=<32+ char secure key>
TMWS_ENVIRONMENT=development|staging|production

# Redis Configuration
TMWS_REDIS_URL=redis://localhost:6379/0

# API Configuration
TMWS_API_HOST=0.0.0.0
TMWS_API_PORT=8000

# Security Settings
TMWS_AUTH_ENABLED=false  # Set true for production
TMWS_RATE_LIMIT_REQUESTS=100
TMWS_RATE_LIMIT_PERIOD=60

# Vector/Embedding Settings
TMWS_EMBEDDING_MODEL=all-MiniLM-L6-v2
TMWS_VECTOR_DIMENSION=384
```

## API Endpoints

### Task Management
- `GET /api/v1/tasks` - List tasks with filters
- `POST /api/v1/tasks` - Create new task
- `GET /api/v1/tasks/{id}` - Get task details
- `PUT /api/v1/tasks/{id}` - Update task
- `DELETE /api/v1/tasks/{id}` - Delete task
- `POST /api/v1/tasks/{id}/complete` - Mark as complete

### Workflow Management
- `GET /api/v1/workflows` - List workflows
- `POST /api/v1/workflows` - Create workflow
- `GET /api/v1/workflows/{id}` - Get workflow details
- `PUT /api/v1/workflows/{id}` - Update workflow
- `DELETE /api/v1/workflows/{id}` - Delete workflow
- `POST /api/v1/workflows/{id}/execute` - Execute workflow
- `POST /api/v1/workflows/{id}/cancel` - Cancel execution
- `GET /api/v1/workflows/{id}/status` - Get execution status

### Memory Operations
- `POST /api/v1/memory/store` - Store semantic memory
- `POST /api/v1/memory/search` - Semantic similarity search
- `GET /api/v1/memory/recall` - Recall memories by criteria
- `DELETE /api/v1/memory/{id}` - Delete memory

### System Health
- `GET /health` - System health check
- `GET /api/v1/stats` - System statistics

## MCP Tools

### Available Tools for Claude Desktop

1. **semantic_search**
   - Search memories using vector similarity
   - Parameters: query, limit, threshold

2. **store_memory**
   - Store new semantic memory
   - Parameters: content, importance, metadata

3. **manage_task**
   - Create, update, delete tasks
   - Parameters: operation, task_data

4. **execute_workflow**
   - Run workflow with parameters
   - Parameters: workflow_id, parameters

## Security Features

### 404 Security Standards
- No default credentials in production
- Mandatory authentication in production
- Cryptographically secure secret keys
- Rate limiting and brute force protection
- Comprehensive audit logging

### Middleware Stack
1. CORS handling with strict origins
2. Redis-based rate limiting
3. JWT authentication (when enabled)
4. Request/response audit logging
5. Security headers (HSTS, CSP, etc.)

## Development Setup

### Quick Start

```bash
# Clone and setup
git clone <repo>
cd tmws
./install.sh

# Configure environment
cp .env.example .env
# Edit .env with your settings

# Initialize database
python -m alembic upgrade head

# Run server
python -m src.main
```

### Testing

```bash
# Unit tests
pytest tests/unit/

# Integration tests
pytest tests/integration/

# Full test suite
pytest tests/ -v --cov=src
```

## Production Deployment

### Requirements
- PostgreSQL 14+ with pgvector extension
- Redis 6+
- Python 3.11+
- 2GB+ RAM
- SSL/TLS certificates

### Security Checklist
- [ ] Set TMWS_ENVIRONMENT=production
- [ ] Set TMWS_AUTH_ENABLED=true
- [ ] Generate secure TMWS_SECRET_KEY
- [ ] Configure CORS origins explicitly
- [ ] Enable SSL on database connections
- [ ] Setup firewall rules
- [ ] Configure reverse proxy (nginx/traefik)
- [ ] Enable audit logging
- [ ] Setup monitoring and alerting

## Integration with Trinitas Agents

TMWS provides the backend infrastructure for Trinitas AI personas:

- **Athena**: Uses workflows for orchestration
- **Artemis**: Leverages task optimization features
- **Hestia**: Utilizes security audit logs
- **Eris**: Manages team coordination tasks
- **Hera**: Executes strategic planning workflows
- **Muses**: Stores and retrieves documentation

Each persona can interact through either REST API or MCP protocol based on the context.

## Recent Updates (2025-01-05)

### Completed Implementation
1. **Full Task/Workflow Routers**: Complete CRUD operations for tasks and workflows
2. **Unified Security Middleware**: Single middleware handling all security aspects
3. **Redis Integration**: Distributed rate limiting and caching
4. **404 Security Standards**: Production-grade security validation
5. **Comprehensive Documentation**: Command reference and system overview

### Architecture Decision
- **FastAPI + FastMCP**: Both are required and complement each other
  - FastAPI: REST endpoints, Swagger UI, external integrations
  - FastMCP: Claude Desktop integration via MCP protocol
- **No backward compatibility concerns**: TMWS v1.0 fresh implementation
- **Simplified design**: Removed duplicate code and unified configuration
# TMWS ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ v3.1

## æ¦‚è¦

TMWS v3.1ã§ã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®6ã¤ã®Trinitasã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«åŠ ãˆã¦ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ã®ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‹•çš„ã«ç™»éŒ²ãƒ»ç®¡ç†ã§ãã¾ã™ã€‚

## ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆã‚·ã‚¹ãƒ†ãƒ æä¾›ï¼‰

| ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ | ID | å½¹å‰² |
|------------|-----|-----|
| **Athena** | athena-conductor | ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®èª¿å’Œçš„ãªæŒ‡æ® |
| **Artemis** | artemis-optimizer | ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¨æŠ€è¡“çš„å“è¶Šæ€§ |
| **Hestia** | hestia-auditor | ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£åˆ†æã¨ç›£æŸ» |
| **Eris** | eris-coordinator | æˆ¦è¡“è¨ˆç”»ã¨ãƒãƒ¼ãƒ èª¿æ•´ |
| **Hera** | hera-strategist | æˆ¦ç•¥è¨ˆç”»ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ |
| **Muses** | muses-documenter | ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆã¨çŸ¥è­˜ç®¡ç† |

## ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç™»éŒ²æ–¹æ³•

### 1. MCPãƒ„ãƒ¼ãƒ«ã«ã‚ˆã‚‹å‹•çš„ç™»éŒ²

```python
# ç ”ç©¶å°‚é–€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç™»éŒ²
register_agent(
  agent_name="researcher",
  full_id="research-specialist",
  capabilities=[
    "literature_review",
    "data_analysis",
    "hypothesis_generation",
    "paper_writing"
  ],
  namespace="academic",
  display_name="Research Specialist",
  access_level="team",
  metadata={
    "specialization": "AI/ML Research",
    "languages": ["English", "Japanese"],
    "tools": ["arxiv", "google_scholar", "semantic_scholar"]
  }
)
```

### 2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹èµ·å‹•æ™‚ç™»éŒ²

`custom_agents.json`:
```json
{
  "version": "1.0",
  "custom_agents": [
    {
      "name": "researcher",
      "full_id": "research-specialist",
      "namespace": "academic",
      "display_name": "Research Specialist",
      "access_level": "team",
      "capabilities": [
        "literature_review",
        "data_analysis",
        "hypothesis_generation"
      ],
      "metadata": {
        "specialization": "AI/ML Research",
        "preferred_personas": ["athena", "muses"]
      }
    },
    {
      "name": "devops",
      "full_id": "devops-engineer",
      "namespace": "infrastructure",
      "display_name": "DevOps Engineer",
      "access_level": "shared",
      "capabilities": [
        "ci_cd_pipeline",
        "container_management",
        "monitoring_setup"
      ]
    }
  ]
}
```

### 3. ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹åˆæœŸè¨­å®š

```bash
# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè‡ªå‹•æ¤œå‡º
export TMWS_AGENT_ID="researcher"
export TMWS_AGENT_NAMESPACE="academic"
export TMWS_AGENT_CAPABILITIES='["research", "analysis", "writing"]'

# Claude Desktopè¨­å®šã§ã‚‚æŒ‡å®šå¯èƒ½
{
  "mcpServers": {
    "tmws": {
      "env": {
        "TMWS_AGENT_ID": "researcher",
        "TMWS_AGENT_NAMESPACE": "academic"
      }
    }
  }
}
```

## ã‚¢ã‚¯ã‚»ã‚¹ãƒ¬ãƒ™ãƒ«

| ãƒ¬ãƒ™ãƒ« | èª¬æ˜ | ä½¿ç”¨ä¾‹ |
|-------|------|--------|
| `private` | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè‡ªèº«ã®ãƒ¡ãƒ¢ãƒªã®ã¿ | å€‹äººä½œæ¥­ç”¨ |
| `team` | åŒã˜namespaceå†…ã§å…±æœ‰ | ãƒãƒ¼ãƒ å†…å”æ¥­ |
| `shared` | æ˜ç¤ºçš„ã«å…±æœ‰ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ | éƒ¨é–€é–“é€£æº |
| `public` | ã™ã¹ã¦ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ | å…¨ç¤¾å…±æœ‰çŸ¥è­˜ |

## å®Ÿè·µä¾‹

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®šç¾©

```python
# QAã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
register_agent(
  agent_name="qa_engineer",
  full_id="quality-assurance-specialist",
  capabilities=[
    "test_planning",
    "test_execution",
    "bug_reporting",
    "automation_scripting"
  ],
  namespace="engineering",
  display_name="QA Specialist",
  access_level="team",
  metadata={
    "test_frameworks": ["pytest", "selenium", "jest"],
    "coverage_target": 0.9,
    "priority_personas": ["hestia", "artemis"]
  }
)

# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
register_agent(
  agent_name="data_scientist",
  full_id="data-science-specialist",
  capabilities=[
    "statistical_analysis",
    "machine_learning",
    "data_visualization",
    "model_evaluation"
  ],
  namespace="analytics",
  display_name="Data Science Specialist",
  access_level="shared",
  metadata={
    "tools": ["pandas", "scikit-learn", "tensorflow"],
    "specialization": "predictive_modeling"
  }
)
```

### ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®å”èª¿

```python
# ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨Trinitasã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å”èª¿ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
create_workflow(
  name="research_to_implementation",
  steps=[
    {"agent": "researcher", "action": "literature_review"},
    {"agent": "hera-strategist", "action": "architecture_design"},
    {"agent": "artemis-optimizer", "action": "implementation"},
    {"agent": "qa_engineer", "action": "testing"},
    {"agent": "muses-documenter", "action": "documentation"}
  ]
)
```

## åˆ¶é™äº‹é …

- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå: 2-32æ–‡å­—ã€è‹±å­—é–‹å§‹ã€è‹±æ•°å­—ãƒ»ãƒã‚¤ãƒ•ãƒ³ãƒ»ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢
- å®Œå…¨ID: 3-64æ–‡å­—
- namespace: æœ€å¤§32æ–‡å­—
- capabilities: æœ€å¤§50å€‹
- metadata: æœ€å¤§10KB

---

# Agent Coordination and Execution Patterns
@AGENTS.md

---
# Generated Information
- Built: 2025-09-08 23:11:42
- Version: v2.1-quadrinity-stable-65-g86f5a6d
- Source: trinitas_sources/common/
---
